{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b902155e",
   "metadata": {},
   "source": [
    "### Simple GEN AI App using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9439a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ecccd3d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "## Data Ingestion -- from the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6568c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1b0cb5f02f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/evaluation\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "021beac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedback using the SDKHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application\\'s intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical metricsHow to simulate multi-turn interactionsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationEvaluation Quick Start\\nEvaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don\\'t always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\\n\\nA dataset with test inputs and optionally expected outputs.\\nA target function that defines what you\\'re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function\\'s outputs.\\n\\nThis quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nSDKUItipThis quickstart uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you\\'re new to evaluations.\\nIf you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.1. Install Dependencies\\u200bPythonTypeScriptpip install -U langsmith openevals openainpm install langsmith openevals openaiinfoIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\\n2. Create a LangSmith API key\\u200bTo create an API key, head to the Settings page. Then click Create API Key.3. Set up your environment\\u200bBecause this quickstart uses OpenAI models, you\\'ll need to set the OPENAI_API_KEY environment variable as well as the\\nrequired LangSmith ones:Shellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langchain-api-key>\"# This example uses OpenAI, but you can use other LLM providers if desiredexport OPENAI_API_KEY=\"<your-openai-api-key>\"4. Create a dataset\\u200bNext, define example input and reference output pairs that you\\'ll use to evaluate your app:PythonTypeScriptfrom langsmith import Clientclient = Client()# Programmatically create a dataset in LangSmith# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_applicationdataset = client.create_dataset(    dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")# Create examplesexamples = [    {        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},    },    {        \"inputs\": {\"question\": \"What is Earth\\'s lowest point?\"},        \"outputs\": {\"answer\": \"Earth\\'s lowest point is The Dead Sea.\"},    },]# Add examples to the datasetclient.create_examples(dataset_id=dataset.id, examples=examples)import { Client } from \"langsmith\";const client = new Client();// Programmatically create a dataset in LangSmith// For other dataset creation methods, see:// https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically// https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_applicationconst dataset = await client.createDataset(\"Sample dataset\", {  description: \"A sample dataset in LangSmith.\",});// Create inputs and reference outputsconst examples = [  {    inputs: { question: \"Which country is Mount Kilimanjaro located in?\" },    outputs: { answer: \"Mount Kilimanjaro is located in Tanzania.\" },    dataset_id: dataset.id,  },  {    inputs: { question: \"What is Earth\\'s lowest point?\" },    outputs: { answer: \"Earth\\'s lowest point is The Dead Sea.\" },    dataset_id: dataset.id,  },];// Add examples to the datasetawait client.createExamples(examples);5. Define what you\\'re evaluating\\u200bNow, define target function that contains what you\\'re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.PythonTypeScriptfrom langsmith import wrappersfrom openai import OpenAI# Wrap the OpenAI client for LangSmith tracingopenai_client = wrappers.wrap_openai(OpenAI())      # Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondef target(inputs: dict) -> dict:    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        messages=[            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},            {\"role\": \"user\", \"content\": inputs[\"question\"]},        ],    )    return { \"answer\": response.choices[0].message.content.strip() }import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: { question: string }): Promise<{ answer: string }> {  const response = await openai.chat.completions.create({    model: \"gpt-4o-mini\",    messages: [      { role: \"system\", content: \"Answer the following question accurately\" },      { role: \"user\", content: inputs.question },    ],  });  return { answer: response.choices[0].message.content?.trim() || \"\" };}6. Define evaluator\\u200bImport a prebuilt prompt from openevals and create an evaluator.\\noutputs are the result of your target function. reference_outputs / referenceOutputs are from the example pairs you defined in step 4 above.infoCORRECTNESS_PROMPT is just an f-string with variables for \"inputs\", \"outputs\", and \"reference_outputs\".\\nSee here for more information on customizing OpenEvals prompts.PythonTypeScriptfrom openevals.llm import create_llm_as_judgefrom openevals.prompts import CORRECTNESS_PROMPTdef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):    evaluator = create_llm_as_judge(        prompt=CORRECTNESS_PROMPT,        model=\"openai:o3-mini\",        feedback_key=\"correctness\",    )    eval_result = evaluator(        inputs=inputs,        outputs=outputs,        reference_outputs=reference_outputs    )    return eval_resultimport { createLLMAsJudge, CORRECTNESS_PROMPT } from \"openevals\";const correctnessEvaluator = async (params: {  inputs: Record<string, unknown>;  outputs: Record<string, unknown>;  referenceOutputs?: Record<string, unknown>;}) => {  const evaluator = createLLMAsJudge({    prompt: CORRECTNESS_PROMPT,    model: \"openai:o3-mini\",    feedbackKey: \"correctness\",  });  const evaluatorResult = await evaluator({    inputs: params.inputs,    outputs: params.outputs,    referenceOutputs: params.referenceOutputs,  });  return evaluatorResult;};7. Run and view results\\u200bFinally, run the experiment!PythonTypeScript# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate(    target,    data=\"Sample dataset\",    evaluators=[        correctness_evaluator,        # can add multiple evaluators here    ],    experiment_prefix=\"first-eval-in-langsmith\",    max_concurrency=2,)import { evaluate } from \"langsmith/evaluation\";// After running the evaluation, a link will be provided to view the results in langsmithawait evaluate(  target,  {    data: \"Sample dataset\",    evaluators: [      correctnessEvaluator,      // can add multiple evaluators here    ],    experimentPrefix: \"first-eval-in-langsmith\",    maxConcurrency: 2,  });Click the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of the experiment.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\nCheck out the OpenEvals README to see all available prebuilt evaluators and how to customize them.\\nLearn how to define custom evaluators that contain arbitrary code.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.\\nOr, if you prefer video tutorials, check out the Datasets, Evaluators, and Experiments videos from the Introduction to LangSmith Course.1. Navigate to the Playground\\u200bLangSmith\\'s Prompt Playground makes it possible to run evaluations over different prompts, new models or test different model configurations. Go to LangSmith\\'s Playground in the UI.2. Create a prompt\\u200bModify the system prompt to:Answer the following question accurately:3. Create a dataset\\u200bClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.Add the following examples to the dataset:InputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earth\\'s lowest point?output: Earth\\'s lowest point is The Dead Sea.Press Save to save your newly created dataset.4. Add an evaluator\\u200bClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.5. Run your evaluation\\u200bPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.See the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nLearn how to create and manage datasets in the UI\\nLearn how to run an evaluation from the prompt playground\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousConceptual GuideNextQuick StartCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c5fb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedback using the SDKHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical metricsHow to simulate multi-turn interactionsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationEvaluation Quick Start'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"A dataset with test inputs and optionally expected outputs.\\nA target function that defines what you're evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function's outputs.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"This quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nSDKUItipThis quickstart uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you're new to evaluations.\\nIf you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.1. Install Dependencies\\u200bPythonTypeScriptpip install -U langsmith openevals openainpm install langsmith openevals openaiinfoIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"2. Create a LangSmith API key\\u200bTo create an API key, head to the Settings page. Then click Create API Key.3. Set up your environment\\u200bBecause this quickstart uses OpenAI models, you'll need to set the OPENAI_API_KEY environment variable as well as the\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='required LangSmith ones:Shellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langchain-api-key>\"# This example uses OpenAI, but you can use other LLM providers if desiredexport OPENAI_API_KEY=\"<your-openai-api-key>\"4. Create a dataset\\u200bNext, define example input and reference output pairs that you\\'ll use to evaluate your app:PythonTypeScriptfrom langsmith import Clientclient = Client()# Programmatically create a dataset in LangSmith# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_applicationdataset = client.create_dataset(    dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\")# Create examplesexamples = [    {        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},    },    {'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='examplesexamples = [    {        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},    },    {        \"inputs\": {\"question\": \"What is Earth\\'s lowest point?\"},        \"outputs\": {\"answer\": \"Earth\\'s lowest point is The Dead Sea.\"},    },]# Add examples to the datasetclient.create_examples(dataset_id=dataset.id, examples=examples)import { Client } from \"langsmith\";const client = new Client();// Programmatically create a dataset in LangSmith// For other dataset creation methods, see:// https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically// https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_applicationconst dataset = await client.createDataset(\"Sample dataset\", {  description: \"A sample dataset in LangSmith.\",});// Create inputs and reference outputsconst examples = [  {    inputs: { question: \"Which country is Mount Kilimanjaro'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='dataset\", {  description: \"A sample dataset in LangSmith.\",});// Create inputs and reference outputsconst examples = [  {    inputs: { question: \"Which country is Mount Kilimanjaro located in?\" },    outputs: { answer: \"Mount Kilimanjaro is located in Tanzania.\" },    dataset_id: dataset.id,  },  {    inputs: { question: \"What is Earth\\'s lowest point?\" },    outputs: { answer: \"Earth\\'s lowest point is The Dead Sea.\" },    dataset_id: dataset.id,  },];// Add examples to the datasetawait client.createExamples(examples);5. Define what you\\'re evaluating\\u200bNow, define target function that contains what you\\'re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.PythonTypeScriptfrom langsmith import wrappersfrom openai import OpenAI# Wrap the OpenAI client for LangSmith tracingopenai_client = wrappers.wrap_openai(OpenAI())      # Define the application logic you want to evaluate inside a'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='import wrappersfrom openai import OpenAI# Wrap the OpenAI client for LangSmith tracingopenai_client = wrappers.wrap_openai(OpenAI())      # Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondef target(inputs: dict) -> dict:    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        messages=[            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},            {\"role\": \"user\", \"content\": inputs[\"question\"]},        ],    )    return { \"answer\": response.choices[0].message.content.strip() }import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: { question: string }):'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasync function target(inputs: { question: string }): Promise<{ answer: string }> {  const response = await openai.chat.completions.create({    model: \"gpt-4o-mini\",    messages: [      { role: \"system\", content: \"Answer the following question accurately\" },      { role: \"user\", content: inputs.question },    ],  });  return { answer: response.choices[0].message.content?.trim() || \"\" };}6. Define evaluator\\u200bImport a prebuilt prompt from openevals and create an evaluator.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='outputs are the result of your target function. reference_outputs / referenceOutputs are from the example pairs you defined in step 4 above.infoCORRECTNESS_PROMPT is just an f-string with variables for \"inputs\", \"outputs\", and \"reference_outputs\".'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='See here for more information on customizing OpenEvals prompts.PythonTypeScriptfrom openevals.llm import create_llm_as_judgefrom openevals.prompts import CORRECTNESS_PROMPTdef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):    evaluator = create_llm_as_judge(        prompt=CORRECTNESS_PROMPT,        model=\"openai:o3-mini\",        feedback_key=\"correctness\",    )    eval_result = evaluator(        inputs=inputs,        outputs=outputs,        reference_outputs=reference_outputs    )    return eval_resultimport { createLLMAsJudge, CORRECTNESS_PROMPT } from \"openevals\";const correctnessEvaluator = async (params: {  inputs: Record<string, unknown>;  outputs: Record<string, unknown>;  referenceOutputs?: Record<string, unknown>;}) => {  const evaluator = createLLMAsJudge({    prompt: CORRECTNESS_PROMPT,    model: \"openai:o3-mini\",    feedbackKey: \"correctness\",  });  const evaluatorResult = await evaluator({    inputs: params.inputs,    outputs: params.outputs,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='prompt: CORRECTNESS_PROMPT,    model: \"openai:o3-mini\",    feedbackKey: \"correctness\",  });  const evaluatorResult = await evaluator({    inputs: params.inputs,    outputs: params.outputs,    referenceOutputs: params.referenceOutputs,  });  return evaluatorResult;};7. Run and view results\\u200bFinally, run the experiment!PythonTypeScript# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate(    target,    data=\"Sample dataset\",    evaluators=[        correctness_evaluator,        # can add multiple evaluators here    ],    experiment_prefix=\"first-eval-in-langsmith\",    max_concurrency=2,)import { evaluate } from \"langsmith/evaluation\";// After running the evaluation, a link will be provided to view the results in langsmithawait evaluate(  target,  {    data: \"Sample dataset\",    evaluators: [      correctnessEvaluator,      // can add multiple evaluators here    ],    experimentPrefix: \"first-eval-in-langsmith\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='evaluate(  target,  {    data: \"Sample dataset\",    evaluators: [      correctnessEvaluator,      // can add multiple evaluators here    ],    experimentPrefix: \"first-eval-in-langsmith\",    maxConcurrency: 2,  });Click the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of the experiment.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='Check out the OpenEvals README to see all available prebuilt evaluators and how to customize them.\\nLearn how to define custom evaluators that contain arbitrary code.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Or, if you prefer video tutorials, check out the Datasets, Evaluators, and Experiments videos from the Introduction to LangSmith Course.1. Navigate to the Playground\\u200bLangSmith's Prompt Playground makes it possible to run evaluations over different prompts, new models or test different model configurations. Go to LangSmith's Playground in the UI.2. Create a prompt\\u200bModify the system prompt to:Answer the following question accurately:3. Create a dataset\\u200bClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.Add the following examples to the dataset:InputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earth's lowest point?output: Earth's lowest point is The Dead Sea.Press Save to save your newly created dataset.4. Add an evaluator\\u200bClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.5. Run your evaluation\\u200bPress Start on the top\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='Save to save your newly created dataset.4. Add an evaluator\\u200bClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.5. Run your evaluation\\u200bPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.See the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='Learn how to create and manage datasets in the UI\\nLearn how to run an evaluation from the prompt playground\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousConceptual GuideNextQuick StartCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load data--> docs --> divide text into chunks -->text--> vectors -->vector embeddings -->vector store db\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "\n",
    "documents=text_splitter.split_documents(docs)\n",
    "\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abb499de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings= OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74796e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e44d1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1b0e4499e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb89e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"Evaluations are made up of three components:\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e72bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64b07a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B108C61A90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B108EF4440>, root_client=<openai.OpenAI object at 0x000001B108F78050>, root_async_client=<openai.AsyncOpenAI object at 0x000001B108F78690>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval chain, Document Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55ea70bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, evaluations consist of a dataset that includes test inputs and may also include expected outputs.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"input\":\"Evaluations are made up of\",\n",
    "        \"context\":[Document(page_content=\"Evaluations are made up of three components:A dataset with test inputs and optionally expected outputs.\")]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc91f9",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7145df",
   "metadata": {},
   "source": [
    "### Input ---> Retriever ---> vectorestoredb\n",
    "\n",
    "retriever is just like a interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99a64715",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "656398f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B0E4499E80>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B108C61A90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B108EF4440>, root_client=<openai.OpenAI object at 0x000001B108F78050>, root_async_client=<openai.AsyncOpenAI object at 0x000001B108F78690>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bf12139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evaluations are a quantitative method to measure the performance of Large Language Model (LLM) applications. They help to identify failures, compare changes across different versions, and build more reliable AI applications. Evaluations consist of various components, including running evaluations with multimodal content, filtering experiments in the UI, and defining different types of evaluators such as LLM-as-a-judge or summary evaluators. They also involve handling aspects such as model rate limits, repetitions, and returning both categorical and numerical metrics. The provided context references tools like the LangSmith Experiments UI and guides on running these evaluations locally or using prebuilt evaluators.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response from the llm\n",
    "\n",
    "response=retrieval_chain.invoke({\"input\":\"Evaluations are made up of three components:\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46f30765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='29e49274-2d56-440a-a092-322f0e09100a', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content=\"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\"),\n",
       " Document(id='896f4133-be15-4396-bb48-77c7a13d9036', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical metricsHow to simulate multi-turn interactionsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationEvaluation Quick Start'),\n",
       " Document(id='eb3b307f-74c7-4316-8463-7a36bbe038ee', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='evaluate(  target,  {    data: \"Sample dataset\",    evaluators: [      correctnessEvaluator,      // can add multiple evaluators here    ],    experimentPrefix: \"first-eval-in-langsmith\",    maxConcurrency: 2,  });Click the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of the experiment.Next steps\\u200btipTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.'),\n",
       " Document(id='40f31bc7-fb08-44a1-8c4b-e2bd0178a2f6', metadata={'source': 'https://docs.smith.langchain.com/evaluation', 'title': 'Evaluation Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': \"Evaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\", 'language': 'en'}, page_content='Check out the OpenEvals README to see all available prebuilt evaluators and how to customize them.\\nLearn how to define custom evaluators that contain arbitrary code.\\nSee the How-to guides for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
