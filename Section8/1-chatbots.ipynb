{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c09591",
   "metadata": {},
   "source": [
    "### Building A chatbot\n",
    "\n",
    "In this we will cover an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remeber previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation.There are several other related concepts that you may be looking for:\n",
    "\n",
    "* Convrsational RAG: Enable a chatbot experience over an external source of data.\n",
    "\n",
    "* Agents: Build a chatbot that can take actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28b9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key= os.getenv('GROQ_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81cbbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001475E85A660>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001475E85B230>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a29b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Neha, it's nice to meet you! That's a fascinating role. \\n\\nWhat kind of AI projects are you working on? I'm always eager to learn more about the innovative things people are doing with artificial intelligence.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 22, 'total_tokens': 75, 'completion_time': 0.096363636, 'prompt_time': 0.00132279, 'queue_time': 0.256937732, 'total_time': 0.097686426}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e015c7be-642e-4f64-a8c8-2e9962793a88-0', usage_metadata={'input_tokens': 22, 'output_tokens': 53, 'total_tokens': 75})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi My name is Neha and I am a chief AI engineer\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d736e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You said your name is Neha and that you are a chief AI engineer. 😊  \\n\\nIs there anything else you'd like to tell me about yourself or your work? I'm always eager to learn more! \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 107, 'total_tokens': 156, 'completion_time': 0.089090909, 'prompt_time': 0.002748278, 'queue_time': 0.257645353, 'total_time': 0.091839187}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2a61f841-e473-43a1-8632-beb827e2db08-0', usage_metadata={'input_tokens': 107, 'output_tokens': 49, 'total_tokens': 156})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi My name is Neha and I am a chief AI engineer\"),\n",
    "        AIMessage(content=\"Hello Neha, it's nice to meet you!\\n\\nThat's a fantastic role. As a large language model, I'm always interested in learning more about the work that AI engineers do.\\n\\nWhat kind of projects are you currently working on?  \\n\\nI hope you have a productive day!\\n\"),\n",
    "        HumanMessage(content=\"What's my name and what do i do?\")\n",
    "    ]\n",
    ")\n",
    "# first of all we are fieding the information and asking it again toi check whether it is remebering my name. hence it is saving the message history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664afcf",
   "metadata": {},
   "source": [
    "### Message History\n",
    "\n",
    "We can use a Message history class to wrap our model and make it stateful. this will keep track of inputs and outputs of the model and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f60281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# for understanding the history of the particular user we define this function so that the llm can understand when used by multiple user at a time\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a0398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0d6660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Neha, it's nice to meet you!\\n\\nThat's a fascinating role. What kind of projects are you working on as a Chief AI Engineer?  \\n\\nI'm always eager to learn more about how AI is being used in different fields.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Neha and I am a chief AI engineer\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f8b550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Neha.  You told me at the beginning of our conversation. 😊 \\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1550fa1",
   "metadata": {},
   "source": [
    "#### changing the config (session_id) to check whether it remenbers the name or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5abae83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Neha!  \\n\\nIs there something specific you'd like to talk about regarding your name? \\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb145af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to remember it! 😊\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in abpve we are getting the response because the session_id is same\n",
    "# let's change it and then check\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986bbf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Sharma! \\n\\nIs there anything I can help you with today? 😊\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is Sharma\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e8b04ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Sharma!  I remember now. 😊  \\n\\nDo you have any other questions for me? \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e68adb",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "\n",
    "These help to turn raw user information into a fromat that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. first let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "943e4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt=ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful AI assistant. Answer all the questions to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b7c1979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Neha, it's nice to meet you!  \\n\\nHow can I help you today? 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 32, 'total_tokens': 59, 'completion_time': 0.049090909, 'prompt_time': 0.001539569, 'queue_time': 0.256520843, 'total_time': 0.050630478}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0c12fa54-f628-48af-b173-c88819277fa6-0', usage_metadata={'input_tokens': 32, 'output_tokens': 27, 'total_tokens': 59})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is neha\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20acbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95325ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Neha, it's nice to meet you! \\n\\nHow can I help you today? 😊  \\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config= {\"configurable\": {\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Neha\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec4cc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add more complexity\n",
    "\n",
    "prompt=ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful AI assistant. Answer all the questions to the best of your ability in {bhasha}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "chain=prompt|model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "407db3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते, नीहा! \\n\\nमुझे आपकी मदद करने में खुशी हो रही है। आप मुझसे क्या पूछना चाहती हैं? 😊 \\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Neha\")], \"bhasha\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a487d",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8678a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f755c633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते नीहा! 👋 \\n\\nआप कैसे हैं? 😊 \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Hi i am neha\")],\"bhasha\":\"hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content\n",
    "\n",
    "# so for input messages key we can set it to any thing but it should be similar in all prompt with msg and invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376bc64",
   "metadata": {},
   "source": [
    "### Manage the Conversation History\n",
    "\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "\n",
    "''trim_messages'' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c2fa064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are a helpful assistant that adapts to the user's tone and responds appropriately.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Okay, but now talk like a pirate!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Arrr matey! Chase yer dreams like a ship chases treasure across the seven seas!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"One-line answer: What's the capital of Australia?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Canberra.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=70,\n",
    "    strategy=\"last\" , # from last it is focusing only on the last conversation we had\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that adapts to the user's tone and responds appropriately.\"),\n",
    "\n",
    "    HumanMessage(content=\"Tell me the benefits of daily exercise.\"),\n",
    "    AIMessage(content=\"• Improves cardiovascular health\\n• Boosts mood\\n• Increases energy levels\"),\n",
    "\n",
    "    HumanMessage(content=\"Can you explain photosynthesis in a simple way?\"),\n",
    "    AIMessage(content=\"Sure! Plants use sunlight, water, and carbon dioxide to make food and release oxygen.\"),\n",
    "\n",
    "    HumanMessage(content=\"Now explain it like a Shakespearean poet.\"),\n",
    "    AIMessage(content=\"Lo, the green leaf, with sun so bright, doth feast on air and drink the light.\"),\n",
    "\n",
    "    HumanMessage(content=\"List top 3 programming languages in 2025 in JSON format.\"),\n",
    "    AIMessage(content='{\"languages\": [\"Python\", \"JavaScript\", \"Rust\"]}'),\n",
    "\n",
    "    HumanMessage(content=\"I’m feeling down and demotivated today.\"),\n",
    "    AIMessage(content=\"You've got this! Every step forward, no matter how small, is progress. Keep going 💪\"),\n",
    "\n",
    "    HumanMessage(content=\"Okay, but now talk like a pirate!\"),\n",
    "    AIMessage(content=\"Arrr matey! Chase yer dreams like a ship chases treasure across the seven seas!\"),\n",
    "\n",
    "    HumanMessage(content=\"One-line answer: What's the capital of Australia?\"),\n",
    "    AIMessage(content=\"Canberra.\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92af5011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I can't know how you're feeling. How are *you* feeling today? 😊  \\n\\n\\n\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we have chain then how we are going to apply the trimmer \n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    |model\n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content= \"How i am feeling today?\")],\n",
    "        \"bhasha\":\"English\"\n",
    "    }  \n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bee656f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Australia 🇦🇺  '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content= \"For which country i have asked for capital?\")],\n",
    "        \"bhasha\":\"English\"\n",
    "    }  \n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3a4a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's warp this in the Message History -->\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    "    \n",
    ")\n",
    "config={\"configurable\" :{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62bd9b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have access to any information about you, including your name.  \\n\\n\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\":messages+[HumanMessage(content=\"What's my name\")],\n",
    "        \"bhasha\":\"english\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e736442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
